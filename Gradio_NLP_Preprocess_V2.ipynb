{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eU92pKjfOmn3"
      },
      "source": [
        "\n",
        "**The Gradio application is best viewed with a light theme (windows) enabled to enhance the readability and visualization of text influence.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygzhF4kUL1im",
        "outputId": "a6837a89-7386-4bdb-b64f-ef273162d16e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gradio as gr\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from captum.attr import LayerIntegratedGradients\n",
        "import html\n",
        "\n",
        "# Load fine-tuned model from local directory\n",
        "model = AutoModelForSequenceClassification.from_pretrained(r\"saved_model1\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(r\"saved_model1\")\n",
        "model.eval()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sLutR51NlSpb"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    text = str(text)\n",
        "    text = text.strip()\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespace\n",
        "    text = re.sub(r'http\\S+|www.\\S+', '', text)  # Remove URLs\n",
        "    text = re.sub(r'\\@[\\w_]+', '', text)  # Remove @mentions\n",
        "    text = re.sub(r'\\#', '', text)  # Remove hashtags symbol\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ITj2999AJJms"
      },
      "outputs": [],
      "source": [
        "# Define forward function for attribution\n",
        "def forward_func(inputs, attention_mask=None):\n",
        "    outputs = model(inputs, attention_mask=attention_mask)\n",
        "    return torch.softmax(outputs.logits, dim=1)[:, 0]  # attribution for class 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUPfkJyWr0xG"
      },
      "source": [
        "#### Color Fix Patch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OE_JoeXerpLu"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import html\n",
        "\n",
        "def interpret(text,confidence):\n",
        "    inputs = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=128\n",
        "    ).to(device)\n",
        "    input_ids = inputs[\"input_ids\"]\n",
        "    attention_mask = inputs[\"attention_mask\"]\n",
        "    ref_input_ids = torch.full_like(input_ids, tokenizer.pad_token_id)\n",
        "\n",
        "    lig = LayerIntegratedGradients(forward_func, model.bert.embeddings)\n",
        "    attributions, delta = lig.attribute(\n",
        "        inputs=input_ids,\n",
        "        baselines=ref_input_ids,\n",
        "        additional_forward_args=(attention_mask,),\n",
        "        return_convergence_delta=True\n",
        "    )\n",
        "\n",
        "    # sum over embedding dims, normalize\n",
        "    scores = attributions.sum(dim=-1).squeeze(0)\n",
        "    scores = scores / (scores.abs().max() + 1e-8)\n",
        "\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
        "\n",
        "    # build legend + wrapper\n",
        "    html_output = \"\"\"\n",
        "    <div style=\"border-top:1px solid #aaa; padding-top:8px; margin-top:8px\">\n",
        "      <b>Legend:</b>\n",
        "      <span style=\"display:inline-block;width:10px;height:10px;background-color:hsl(0,75%,50%);border:1px solid;\"></span> Negative \n",
        "      <span style=\"display:inline-block;width:10px;height:10px;background-color:hsl(0,0%,100%);border:1px solid;\"></span> Neutral \n",
        "      <span style=\"display:inline-block;width:10px;height:10px;background-color:hsl(120,75%,50%);border:1px solid;\"></span> Positive\n",
        "      <br><br>\n",
        "    \"\"\"\n",
        "\n",
        "    for score, token in zip(scores.tolist(), tokens):\n",
        "        if token in tokenizer.all_special_tokens:\n",
        "            continue\n",
        "        # clean up BPE token\n",
        "        display_token = token.replace(\"Ġ\", \" \")\n",
        "        if not display_token.startswith(\" \"):\n",
        "            display_token = \" \" + display_token\n",
        "\n",
        "        # clamp & compute hue/lightness\n",
        "        if confidence == \"suicidal_prob\":\n",
        "            s = max(-1, min(1, score))  # should already be in [-1,1]\n",
        "            hue = 0 if s < 0 else 120\n",
        "            lightness = 100 - abs(s) * 50  # 0→white(100%), 1→50%\n",
        "            color = f\"hsl({hue},75%,{lightness:.0f}%)\"\n",
        "\n",
        "        else :\n",
        "            s = max(-1, min(1, score))  # should already be in [-1,1]\n",
        "            hue = 0 if s > 0 else 120\n",
        "            lightness = 100 - abs(s) * 50  # 0→white(100%), 1→50%\n",
        "            color = f\"hsl({hue},75%,{lightness:.0f}%)\"\n",
        "\n",
        "        html_output += f\"<mark style='background-color:{color}; padding:2px; margin:1px'><font color='black'>{html.escape(display_token)}</font></mark>\"\n",
        "\n",
        "    html_output += \"</div>\"\n",
        "    return html_output\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The model shows a bias toward non-suicidal predictions; therefore, a lower threshold of 0.65 is set for the non-suicidal class, as misclassifications are frequently observed within this range."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmN-NvL4c1Ku"
      },
      "source": [
        "**If want to Debug, adjust gr.Interface().launch(debug=False) to True**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 633
        },
        "id": "PxwZpSxyJkcw",
        "outputId": "13e966a1-d876-43cb-947f-bd4e3ea710ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "* To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ID to label mapping\n",
        "label_map = {0: \"❌Suicidal\", 1: \"❇️Non-Suicidal\"}\n",
        "\n",
        "# Predict and explain\n",
        "def predict_and_explain(text, threshold):\n",
        "    text = clean_text(text)  # <---- Pre-cleaning added\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "        pred = torch.argmax(probs, dim=1).item()\n",
        "        # confidence = probs[0][pred].item()\n",
        "\n",
        "        non_suicidal_prob = probs[0][1].item()\n",
        "        suicidal_prob = probs[0][0].item()\n",
        "\n",
        "        if non_suicidal_prob < threshold:\n",
        "            pred = 0\n",
        "            confidence = suicidal_prob\n",
        "        else:\n",
        "            pred = 1\n",
        "            confidence = non_suicidal_prob\n",
        "\n",
        "    result = f\"{label_map[pred]} (Confidence: {confidence:.2%})\"\n",
        "    explanation = interpret(text,confidence)\n",
        "    return result, explanation\n",
        "\n",
        "# Gradio UI\n",
        "gr.Interface(\n",
        "    fn=predict_and_explain,\n",
        "    inputs=[\n",
        "        gr.Textbox(lines=4, placeholder=\"Enter text here...\", label=\"Input Text\"),\n",
        "        gr.Slider(0.5, 0.95, value=0.65, step=0.01, label=\"Non-suicidal Confidence Threshold\")\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Prediction\"),\n",
        "        gr.HTML(label=\"Model Explanation (token importance)\")\n",
        "    ],\n",
        "    title=\"Text Classification for Mental Health with Explanation\",\n",
        "    description=\"This demo uses a fine-tuned BERT model to classify text and explain predictions using Integrated Gradients.\"\n",
        ").launch(debug=False)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (spacy_env)",
      "language": "python",
      "name": "spacy_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
